# 📌 1단계: 선형 회귀 (Linear Regression)란?

## (1) 회귀(Regression)란?
머신러닝에서 **주어진 데이터를 기반으로 숫자를 예측하는 문제**를 **회귀(Regression)** 라고 해.

예를 들어,  
- **X = 공부한 시간**  
- **Y = 시험 점수**  
이런 데이터가 있다고 하자.  

우리는 **어떤 사람이 공부한 시간을 입력하면, 시험 점수를 예측하는 모델을 만들고 싶어!**  
이때 사용하는 방법이 **선형 회귀(Linear Regression)** 야.

## (2) 선형 회귀는 어떻게 동작할까?
선형 회귀는 **가장 잘 맞는 직선을 찾아서, 이를 이용해 값을 예측하는 방법**이야.

아래는 공부 시간(X)과 시험 성적(Y) 간의 관계를 예측하기 위해 진행한 선형 회귀의 예시야  
파란색 점은 실제 데이터고, 빨간색 점은 파란색 점으로 예측한 직선이야. 

![alt text](image.png)

이 직선은 **수식으로 표현하면 다음과 같아**:


$\hat{y} = WX + B$


|기호|의미|
|---|---|
|$X$|입력 데이터 (독립변수)|
|$\hat{y}$|예측값 (종속변수)|
|$W$|가중치(weight), 직선의 기울기|
|$B$|편향(bias), 직선의 절편|

👉 **이제 목표는?**  
👉 **이 직선을 가장 적절하게 조정해서, 예측값과 실제값의 차이를 최소화하는 것!**  
👉 **즉, 적절한 $W$ 와 $B$ 를 찾는 것!**  

이를 위해 **손실 함수**를 정의해야 해.

---

# 📌 2단계: 손실 함수(Loss Function)란?

## (1) 손실 함수란?
모델이 데이터를 얼마나 잘 예측하는지를 측정하는 함수야.  
즉, **모델이 얼마나 틀렸는지를 수치화해서 보여주는 함수**라고 보면 돼.

우리가 찾은 직선이 데이터를 완벽하게 예측할 수는 없으니까, **예측값과 실제값의 차이(오차, Error)** 를 최소화해야 해.

---

## (2) 평균제곱오차(MSE, Mean Squared Error)
선형 회귀에서는 손실 함수로 **평균제곱오차(MSE)** 를 사용해.


$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$


여기서:
- $y_i$ = 실제 값  
- $\hat{y}_i$ = 예측 값  
- $n$ = 데이터 개수  

👉 **즉, 예측값과 실제값의 차이를 제곱해서 평균을 낸 값이 MSE야!**  

왜 제곱을 하냐고? 🤔  
- 음수/양수를 없애서 차이를 절댓값으로 만들기 위해!  
- 작은 차이는 더 작게, 큰 차이는 더 크게 반영하기 위해!  

---

## (3) 손실 함수의 그래프
손실 함수(MSE)가 어떻게 변하는지 그래프로 그려볼게!  
(예제: 가중치 $W$ 에 따라 손실 함수가 어떻게 변하는지)

![alt text](image-1.png)

---

# 📌 3단계: 경사 하강법(Gradient Descent)이란?

## **1️⃣ 경사 하강법이란?**
경사 하강법(Gradient Descent)은 **손실 함수(Loss Function)를 최소화하기 위해 가중치(Weight)를 점진적으로 업데이트하는 최적화 알고리즘**이야.

머신러닝 모델이 학습하는 과정은 결국, **"손실을 줄이는 방향으로 가중치를 조정하는 것"**이 핵심이야.  
이걸 자동으로 수행하는 게 **경사 하강법(Gradient Descent)!**

---

## **2️⃣ 손실 함수(Loss Function)란?**
손실 함수는 모델의 예측값과 실제값의 차이를 평가하는 함수야.  
즉, **모델이 얼마나 틀렸는지를 수치화해서 표현하는 함수**라고 보면 돼.

### 🔹 **선형 회귀(Linear Regression)에서의 손실 함수**
선형 회귀에서 가장 많이 사용하는 손실 함수는 **평균제곱오차(MSE, Mean Squared Error)**야.

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

여기서:
- $y_i$ = 실제값
- $\hat{y}_i$ = 모델이 예측한 값
- $n$ = 데이터 개수

손실 함수의 목표는 **이 값을 최소화하는 최적의 가중치 $W$를 찾는 것!**

---

## **3️⃣ 경사 하강법의 동작 원리**
경사 하강법은 **손실 함수의 기울기(Gradient)를 이용해서 가중치 $W$를 업데이트하는 방식**으로 작동해.

### 🔹 **경사 하강법 업데이트 공식**
$$
W = W - \alpha \cdot \frac{\partial L}{\partial W}
$$

- $W$ = 현재 가중치 (Weight)
- $\alpha$ = 학습률 (Learning Rate), 한 번에 이동하는 크기
- $\frac{\partial L}{\partial W}$ = 손실 함수 $L(W)$을 가중치 $W$에 대해 미분한 값 (기울기)

👉 **즉, 손실 함수의 기울기가 가파르면 크게 이동하고, 작으면 천천히 이동하면서 최적의 $W$를 찾아가는 방식!**

---

## **4️⃣ 경사 하강법 종류**
| 방법 | 한 번의 업데이트에 사용하는 데이터 개수 | 특징 |
|------|-----------------------------------|------|
| **배치 경사 하강법 (Batch GD)** | 전체 데이터 사용 ($n$개) | 학습이 안정적이지만 느림 |
| **확률적 경사 하강법 (SGD)** | 1개씩 사용 ($1$개) | 빠르지만 불안정 |
| **미니배치 경사 하강법 (Mini-batch GD)** | 일부 데이터 사용 ($k$개) | 속도와 안정성의 균형 |

---

## **5️⃣ 배치 경사 하강법 (Batch Gradient Descent)**

### 🔹 **배치 경사 하강법의 과정**
1. 모든 데이터를 사용하여 손실 함수 값을 계산
$$
L(W) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

2. 모든 데이터의 평균 기울기를 계산
$$
\frac{\partial L}{\partial W} = - \frac{2}{n} \sum_{i=1}^{n} X_i (y_i - \hat{y}_i)
$$

3. 가중치 $W$ 업데이트
$$
W = W - \alpha \cdot \frac{\partial L}{\partial W}
$$

✅ **장점**: 학습이 안정적이고 최적의 방향을 잘 찾음  
❌ **단점**: 데이터가 너무 많으면 연산량이 커져 학습이 느려짐

---

## **6️⃣ 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)**

### 🔹 **SGD의 과정**
1. **랜덤한 데이터 1개**를 선택하여 손실을 계산
$$
L(W) = (y_i - \hat{y}_i)^2
$$

2. 해당 데이터 하나의 기울기만 사용하여 가중치 업데이트
$$
W = W - \alpha \cdot (-2 X_i (y_i - \hat{y}_i))
$$

✅ **장점**: 업데이트가 빠름, 메모리 사용량이 적음  
❌ **단점**: 결과가 불안정하고 진동이 심함

---

## **7️⃣ 미니배치 경사 하강법 (Mini-batch Gradient Descent)**

미니배치 경사 하강법은 **배치(Batch)와 확률적(SGD) 방식의 장점을 조합한 방법**이야.

- 데이터를 일정 크기(예: 32개)로 나눈 후 학습.
- 데이터 개수에 따라 적절한 크기의 미니배치를 선택하면 속도와 안정성을 모두 잡을 수 있음.

✅ **장점**: 연산 속도가 빠르고, 안정적임  
❌ **단점**: 배치 크기 조절이 필요함

---

## **8️⃣ 배치 vs 확률적 vs 미니배치 경사 하강법 비교**

### 🔹 **업데이트 방식 비교**
- **배치 경사 하강법**: 모든 데이터를 사용해 한 번만 업데이트
- **확률적 경사 하강법 (SGD)**: 랜덤한 데이터 하나로 즉시 업데이트
- **미니배치 경사 하강법**: 일부 데이터를 사용해서 적절한 크기로 업데이트

### 🔹 **학습 속도 및 안정성 비교**
- **배치 경사 하강법**: 가장 안정적이지만 느림
- **확률적 경사 하강법 (SGD)**: 빠르지만 불안정
- **미니배치 경사 하강법**: 균형 잡힌 학습 방식

---

## **9️⃣ 결론**
- 데이터가 작다면 **배치 경사 하강법**이 효과적!
- 데이터가 크다면 **SGD는 너무 흔들려서 불안정**할 수 있음.
- 그래서 **Mini-batch Gradient Descent가 현실적으로 가장 많이 사용됨!** 🚀

# 🚀 경사 하강법을 로지스틱 회귀(Logistic Regression)로 확장하기

지금까지 우리는 **선형 회귀(Linear Regression)**와 **경사 하강법(Gradient Descent)**을 배웠어. 이제 이 개념을 **분류(Classification) 문제**를 해결하는 **로지스틱 회귀(Logistic Regression)**로 더 자세히 확장해보자!

---

## 📌 **1️⃣ 선형 회귀에서 분류 문제로의 확장**

### 🔹 **선형 회귀의 한계**
선형 회귀는 연속적인 숫자 값을 예측할 때는 좋지만, 분류처럼 특정 카테고리(예: 합격/불합격, 성공/실패)를 예측하기에는 한계가 있어.

예를 들어, 시험 점수를 가지고 합격(1)과 불합격(0)을 구분하려면, 선형 회귀로 예측한 값이 반드시 0과 1 사이에 있어야 하는데, 선형 회귀는 이 범위를 벗어난 값을 예측할 수도 있어.

**예시:**
$$
\hat{y} = WX + B
$$
- 만약 점수가 매우 높다면 $\hat{y}=1.2$ (확률은 100%를 넘어갈 수 없어!)
- 만약 점수가 낮다면 $\hat{y}=-0.4$ (확률은 음수가 될 수 없어!)

👉 **그래서 우리는 결과를 반드시 0과 1 사이의 확률로 제한하는 방법이 필요해!**

---

## 📌 **2️⃣ 시그모이드 함수(Sigmoid Function)**
로지스틱 회귀는 이러한 문제를 해결하기 위해 **시그모이드 함수**를 사용해 선형 회귀 결과를 0과 1 사이의 확률로 변환해.

### 🔹 **시그모이드 함수 정의**
시그모이드 함수는 입력되는 어떤 값이든 항상 0과 1 사이의 값으로 변환하는 특별한 함수야.

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

- 여기서 $z = WX + B$ (선형 회귀의 출력 값)

### 🔹 **시그모이드 함수의 특징**
- $z$가 크면 클수록 값은 1에 가까워지고, 작으면 작을수록 0에 가까워져.
- 따라서 모델이 예측한 값을 직관적으로 확률처럼 해석할 수 있게 해줘.

| $z$ 값         | $\sigma(z)$ 결과      | 의미             |
|--------------|------------------|----------------|
| $z \to -\infty$ | 0 (0%)           | 거의 불가능에 가까운 확률 |
| $z=0$        | 0.5 (50%)        | 정확히 절반의 확률      |
| $z \to +\infty$ | 1 (100%)         | 거의 확실한 확률      |

---

## 📌 **3️⃣ 로지스틱 회귀의 손실 함수**
이제 우리는 모델이 얼마나 잘 예측하고 있는지를 평가할 방법이 필요해. 여기서는 평균제곱오차(MSE) 대신, **크로스 엔트로피 손실 함수**를 사용해.

### 🔹 **크로스 엔트로피 손실 함수 정의**
크로스 엔트로피는 실제값과 예측값의 차이를 확률적으로 잘 나타내주는 손실 함수야.

![alt text](image-2.png)

$$
L(W) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
$$

- $y_i$: 실제 정답(0 또는 1)
- $\hat{y}_i$: 예측 확률값 (시그모이드 함수의 출력)

### 🔹 **왜 크로스 엔트로피를 사용할까?**
- 실제 정답이 1인데 모델이 매우 낮은 확률로 예측했다면 손실 값이 매우 커져서 모델이 이를 수정하도록 압박해.
- 반대로 실제 정답이 0인데 모델이 높은 확률로 예측하면 역시 손실 값이 매우 커져 모델이 빠르게 수정하도록 유도해.

즉, 모델이 잘못된 예측을 "확신할수록" 페널티가 커져서 빠르게 오류를 수정할 수 있도록 돕는 거야.

---

## 📌 **4️⃣ 로지스틱 회귀에서 경사 하강법 적용하기**
이제 크로스 엔트로피 손실을 최소화하기 위해 경사 하강법을 적용하는 방법을 알아보자!

### 🔹 **크로스 엔트로피 손실 함수 미분하기**
손실 함수를 미분하면 모델이 어디로 이동해야 할지 방향이 정해져.

$$
\frac{\partial L}{\partial W} = \frac{1}{n} \sum_{i=1}^{n} X_i (\sigma(WX_i + B) - y_i)
$$

- 이 미분값은 예측값($\sigma(WX_i + B)$)과 실제값($y_i$) 사이의 오차를 고려해서 방향을 제시해줘.

### 🔹 **경사 하강법 가중치 업데이트 공식**
손실을 줄이는 방향으로 가중치를 반복해서 업데이트하면 점점 모델이 정확해져!

$$
W = W - \alpha \cdot \frac{\partial L}{\partial W}
$$

- $\alpha$: 학습률(learning rate), 업데이트 크기를 결정하는 값

---

# 🚀 선형 회귀에서 이진 분류를 거쳐 다중 클래스 분류까지 (초심자용)

지금까지 배운 **선형 회귀(Linear Regression)**에서 출발하여, 이 개념이 어떻게 **이진 분류(Binary Classification)**를 거쳐 **다중 클래스 분류(Multi-Class Classification)**까지 확장되는지 아주 자세히 살펴보자!

---

## 📌 **1️⃣ 선형 회귀에서 이진 분류로 확장**

### 🔹 **선형 회귀란?**
선형 회귀는 입력값을 통해 **연속적인 숫자**를 예측하는 방법이야.

**공식:**
$$
\hat{y} = WX + B
$$

### 🔹 **이진 분류란?**
이진 분류는 두 가지 클래스 중 하나(예: 합격/불합격, 성공/실패)를 예측하는 문제야.

그런데 선형 회귀를 그대로 이진 분류에 쓰면 **문제**가 있어:
- 예측값이 0과 1을 벗어날 수 있음.
- 확률로 해석하기 어려움.

### 🔹 **해결책: 시그모이드 함수(Sigmoid)**
그래서 선형 회귀 결과를 **시그모이드**라는 함수로 변환해 확률(0~1)로 제한하게 돼:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}, \quad z=WX+B
$$

이제 예측값은 항상 **0~1 사이의 확률**로 변환돼!

---

## 📌 **2️⃣ 이진 분류에서 다중 클래스 분류로 확장**

### 🔹 **이진 분류의 한계**
이진 분류는 두 개의 클래스만 구별할 수 있어. 만약 클래스가 3개 이상이라면, 시그모이드로 해결할 수 없어!

### 🔹 **다중 클래스 분류 문제**
이런 문제를 해결하려면, 각 클래스가 선택될 확률을 동시에 계산하는 방법이 필요해.

예를 들어 클래스가 **고양이🐱, 강아지🐶, 토끼🐰** 총 3개라면?

- 각각의 클래스가 선택될 확률을 동시에 알아야 해.
- 확률의 합은 반드시 1이 되어야 해.

### 🔹 **해결책: 소프트맥스 함수(Softmax)**
이때 등장하는 함수가 바로 **소프트맥스**야:

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

소프트맥스는 모든 클래스의 확률을 동시에 계산하고, 전체 합이 1이 되도록 해줘!

---

## 📌 **3️⃣ 다중 클래스 분류의 작동 방식 (선형 회귀 → 소프트맥스)**

### 🔹 **클래스별 선형 회귀 수행**
다중 클래스 분류는 각 클래스마다 별도의 선형 회귀를 수행해 점수(로짓, Logit)를 계산해:

예:
- 고양이🐱 클래스 로짓: $z_{cat} = W_{cat}X + B_{cat}$
- 강아지🐶 클래스 로짓: $z_{dog} = W_{dog}X + B_{dog}$
- 토끼🐰 클래스 로짓: $z_{rabbit} = W_{rabbit}X + B_{rabbit}$

즉, **각 클래스마다 독립적인 선형 회귀 모델이 존재**하고, 입력 데이터가 얼마나 각 클래스에 가까운지를 점수로 표현하는 거지.

### 🔹 **소프트맥스 함수로 확률 변환**
이렇게 나온 각 클래스의 로짓을 소프트맥스 함수로 입력하여 확률로 변환해:

$$
P(y=cat|X) = \frac{e^{z_{cat}}}{e^{z_{cat}} + e^{z_{dog}} + e^{z_{rabbit}}}
$$

각 클래스에 대해 이 과정을 수행하면 결과적으로 각 클래스가 될 확률이 계산돼:
- $P(cat|X)$, $P(dog|X)$, $P(rabbit|X)$
- 확률들의 합은 항상 1이 됨!

---

## 📌 **4️⃣ 다중 클래스 크로스 엔트로피 손실 함수**

다중 클래스 분류에서 모델의 성능을 평가하는 손실 함수는 다중 클래스 크로스 엔트로피 손실 함수야:

$$
L(W) = -\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{C}y_{ij}\log(\hat{y}_{ij})
$$

- $y_{ij}$: 실제 정답(One-hot Encoding)
- $\hat{y}_{ij}$: 예측 확률값(소프트맥스 결과)

### 🔹 **One-hot Encoding이란?**
정답 클래스를 1로, 나머지 클래스는 0으로 표현하는 방법:

예시:
- 정답이 강아지🐶라면: [0, 1, 0]
- 정답이 토끼🐰라면: [0, 0, 1]

이렇게 하면 실제값과 예측값의 비교가 명확해져.

---

## 📌 **5️⃣ 최종 정리**

1. **선형 회귀**로 연속 값을 예측
2. **시그모이드 함수**로 이진 분류 문제의 확률 예측
3. **소프트맥스 함수**로 다중 클래스 분류의 확률 예측
4. 클래스 개수만큼 독립된 **선형 회귀 모델**로 로짓을 계산하고, 소프트맥스로 확률로 변환
5. 크로스 엔트로피 손실 함수를 사용하여 모델을 평가하고, **경사 하강법**으로 최적화

이제 선형 회귀에서 이진 분류를 거쳐 다중 클래스 분류까지의 흐름을 자세히 이해했어! 🎉🚀



---





